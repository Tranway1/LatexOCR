{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "LaTeX OCR test.ipynb",
   "provenance": [],
   "collapsed_sections": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# LaTeX OCR\n",
    "In this colab you can convert an image of an equation into LaTeX code.\n",
    "## How?\n",
    "Execute the cell titled \"Setup\". The first time an error will show up. Simply execute the cell again. Everything should be fine now.\n",
    "\n",
    "Next, execute the cell below and upload the image(s).\n",
    "\n",
    "> Note: You can probably also run this project locally and with a GUI. Follow the steps on [GitHub](https://github.com/lukas-blecher/LaTeX-OCR)"
   ],
   "metadata": {
    "id": "aaAqi3wku23I",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "form",
    "id": "DQM_PKeCuzWR",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[31m  ERROR: Command errored out with exit status 1:\r\n",
      "   command: /Users/chunwei/research/LaTeX-OCR/venv/bin/python /Users/chunwei/research/LaTeX-OCR/venv/lib/python3.9/site-packages/pip/_vendor/pep517/in_process/_in_process.py build_wheel /var/folders/87/jh102m3n0z7_j_46hrcghhyc0000gn/T/tmpoz7aay3o\r\n",
      "       cwd: /private/var/folders/87/jh102m3n0z7_j_46hrcghhyc0000gn/T/pip-install-yglc6eh2/tokenizers_3c98800bb8334f4ca18ea2fa2b2a1380\r\n",
      "  Complete output (58 lines):\r\n",
      "  running bdist_wheel\r\n",
      "  running build\r\n",
      "  running build_py\r\n",
      "  creating build\r\n",
      "  creating build/lib.macosx-10.9-universal2-cpython-39\r\n",
      "  creating build/lib.macosx-10.9-universal2-cpython-39/tokenizers\r\n",
      "  copying py_src/tokenizers/__init__.py -> build/lib.macosx-10.9-universal2-cpython-39/tokenizers\r\n",
      "  creating build/lib.macosx-10.9-universal2-cpython-39/tokenizers/models\r\n",
      "  copying py_src/tokenizers/models/__init__.py -> build/lib.macosx-10.9-universal2-cpython-39/tokenizers/models\r\n",
      "  creating build/lib.macosx-10.9-universal2-cpython-39/tokenizers/decoders\r\n",
      "  copying py_src/tokenizers/decoders/__init__.py -> build/lib.macosx-10.9-universal2-cpython-39/tokenizers/decoders\r\n",
      "  creating build/lib.macosx-10.9-universal2-cpython-39/tokenizers/normalizers\r\n",
      "  copying py_src/tokenizers/normalizers/__init__.py -> build/lib.macosx-10.9-universal2-cpython-39/tokenizers/normalizers\r\n",
      "  creating build/lib.macosx-10.9-universal2-cpython-39/tokenizers/pre_tokenizers\r\n",
      "  copying py_src/tokenizers/pre_tokenizers/__init__.py -> build/lib.macosx-10.9-universal2-cpython-39/tokenizers/pre_tokenizers\r\n",
      "  creating build/lib.macosx-10.9-universal2-cpython-39/tokenizers/processors\r\n",
      "  copying py_src/tokenizers/processors/__init__.py -> build/lib.macosx-10.9-universal2-cpython-39/tokenizers/processors\r\n",
      "  creating build/lib.macosx-10.9-universal2-cpython-39/tokenizers/trainers\r\n",
      "  copying py_src/tokenizers/trainers/__init__.py -> build/lib.macosx-10.9-universal2-cpython-39/tokenizers/trainers\r\n",
      "  creating build/lib.macosx-10.9-universal2-cpython-39/tokenizers/implementations\r\n",
      "  copying py_src/tokenizers/implementations/byte_level_bpe.py -> build/lib.macosx-10.9-universal2-cpython-39/tokenizers/implementations\r\n",
      "  copying py_src/tokenizers/implementations/sentencepiece_unigram.py -> build/lib.macosx-10.9-universal2-cpython-39/tokenizers/implementations\r\n",
      "  copying py_src/tokenizers/implementations/sentencepiece_bpe.py -> build/lib.macosx-10.9-universal2-cpython-39/tokenizers/implementations\r\n",
      "  copying py_src/tokenizers/implementations/base_tokenizer.py -> build/lib.macosx-10.9-universal2-cpython-39/tokenizers/implementations\r\n",
      "  copying py_src/tokenizers/implementations/__init__.py -> build/lib.macosx-10.9-universal2-cpython-39/tokenizers/implementations\r\n",
      "  copying py_src/tokenizers/implementations/char_level_bpe.py -> build/lib.macosx-10.9-universal2-cpython-39/tokenizers/implementations\r\n",
      "  copying py_src/tokenizers/implementations/bert_wordpiece.py -> build/lib.macosx-10.9-universal2-cpython-39/tokenizers/implementations\r\n",
      "  creating build/lib.macosx-10.9-universal2-cpython-39/tokenizers/tools\r\n",
      "  copying py_src/tokenizers/tools/__init__.py -> build/lib.macosx-10.9-universal2-cpython-39/tokenizers/tools\r\n",
      "  copying py_src/tokenizers/tools/visualizer.py -> build/lib.macosx-10.9-universal2-cpython-39/tokenizers/tools\r\n",
      "  copying py_src/tokenizers/__init__.pyi -> build/lib.macosx-10.9-universal2-cpython-39/tokenizers\r\n",
      "  copying py_src/tokenizers/models/__init__.pyi -> build/lib.macosx-10.9-universal2-cpython-39/tokenizers/models\r\n",
      "  copying py_src/tokenizers/decoders/__init__.pyi -> build/lib.macosx-10.9-universal2-cpython-39/tokenizers/decoders\r\n",
      "  copying py_src/tokenizers/normalizers/__init__.pyi -> build/lib.macosx-10.9-universal2-cpython-39/tokenizers/normalizers\r\n",
      "  copying py_src/tokenizers/pre_tokenizers/__init__.pyi -> build/lib.macosx-10.9-universal2-cpython-39/tokenizers/pre_tokenizers\r\n",
      "  copying py_src/tokenizers/processors/__init__.pyi -> build/lib.macosx-10.9-universal2-cpython-39/tokenizers/processors\r\n",
      "  copying py_src/tokenizers/trainers/__init__.pyi -> build/lib.macosx-10.9-universal2-cpython-39/tokenizers/trainers\r\n",
      "  copying py_src/tokenizers/tools/visualizer-styles.css -> build/lib.macosx-10.9-universal2-cpython-39/tokenizers/tools\r\n",
      "  running build_ext\r\n",
      "  running build_rust\r\n",
      "  cargo rustc --lib --message-format=json-render-diagnostics --manifest-path Cargo.toml --release -v --features pyo3/extension-module -- --crate-type cdylib -C link-args=-undefined dynamic_lookup -Wl,-install_name,@rpath/tokenizers.cpython-39-darwin.so\r\n",
      "      Updating crates.io index\r\n",
      "  warning: spurious network error (2 tries remaining): failed open - '/Users/chunwei/.cargo/registry/index/github.com-1ecc6299db9ec823/.git/FETCH_HEAD' is locked: Permission denied; class=Os (2); code=Locked (-14)\r\n",
      "  warning: spurious network error (1 tries remaining): failed open - '/Users/chunwei/.cargo/registry/index/github.com-1ecc6299db9ec823/.git/FETCH_HEAD' is locked: Permission denied; class=Os (2); code=Locked (-14)\r\n",
      "  error: failed to get `env_logger` as a dependency of package `tokenizers-python v0.11.0 (/private/var/folders/87/jh102m3n0z7_j_46hrcghhyc0000gn/T/pip-install-yglc6eh2/tokenizers_3c98800bb8334f4ca18ea2fa2b2a1380)`\r\n",
      "  \r\n",
      "  Caused by:\r\n",
      "    failed to load source for dependency `env_logger`\r\n",
      "  \r\n",
      "  Caused by:\r\n",
      "    Unable to update registry `crates-io`\r\n",
      "  \r\n",
      "  Caused by:\r\n",
      "    failed to fetch `https://github.com/rust-lang/crates.io-index`\r\n",
      "  \r\n",
      "  Caused by:\r\n",
      "    failed open - '/Users/chunwei/.cargo/registry/index/github.com-1ecc6299db9ec823/.git/FETCH_HEAD' is locked: Permission denied; class=Os (2); code=Locked (-14)\r\n",
      "  error: `cargo rustc --lib --message-format=json-render-diagnostics --manifest-path Cargo.toml --release -v --features pyo3/extension-module -- --crate-type cdylib -C 'link-args=-undefined dynamic_lookup -Wl,-install_name,@rpath/tokenizers.cpython-39-darwin.so'` failed with code 101\r\n",
      "  ----------------------------------------\u001B[0m\r\n",
      "\u001B[31m  ERROR: Failed building wheel for tokenizers\u001B[0m\r\n",
      "\u001B[31mERROR: Could not build wheels for tokenizers, which is required to install pyproject.toml-based projects\u001B[0m\r\n",
      "\u001B[31mERROR: Could not find a version that satisfies the requirement opencv-python-headless==4.1.2.30 (from versions: 3.4.10.37, 3.4.11.39, 3.4.11.41, 3.4.11.43, 3.4.11.45, 3.4.13.47, 3.4.15.55, 3.4.16.57, 3.4.16.59, 3.4.17.61, 3.4.17.63, 3.4.18.65, 4.3.0.38, 4.4.0.40, 4.4.0.42, 4.4.0.44, 4.4.0.46, 4.5.1.48, 4.5.3.56, 4.5.4.58, 4.5.4.60, 4.5.5.62, 4.5.5.64, 4.6.0.66, 4.7.0.68)\u001B[0m\r\n",
      "\u001B[31mERROR: No matching distribution found for opencv-python-headless==4.1.2.30\u001B[0m\r\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for Model:\n\tMissing key(s) in state_dict: \"decoder.net.token_emb.emb.weight\", \"decoder.net.attn_layers.layers.0.0.0.weight\", \"decoder.net.attn_layers.layers.0.0.0.bias\", \"decoder.net.attn_layers.layers.1.0.0.weight\", \"decoder.net.attn_layers.layers.1.0.0.bias\", \"decoder.net.attn_layers.layers.2.0.0.weight\", \"decoder.net.attn_layers.layers.2.0.0.bias\", \"decoder.net.attn_layers.layers.2.1.ff.0.proj.weight\", \"decoder.net.attn_layers.layers.2.1.ff.0.proj.bias\", \"decoder.net.attn_layers.layers.2.1.ff.3.weight\", \"decoder.net.attn_layers.layers.2.1.ff.3.bias\", \"decoder.net.attn_layers.layers.3.0.0.weight\", \"decoder.net.attn_layers.layers.3.0.0.bias\", \"decoder.net.attn_layers.layers.4.0.0.weight\", \"decoder.net.attn_layers.layers.4.0.0.bias\", \"decoder.net.attn_layers.layers.5.0.0.weight\", \"decoder.net.attn_layers.layers.5.0.0.bias\", \"decoder.net.attn_layers.layers.5.1.ff.0.proj.weight\", \"decoder.net.attn_layers.layers.5.1.ff.0.proj.bias\", \"decoder.net.attn_layers.layers.5.1.ff.3.weight\", \"decoder.net.attn_layers.layers.5.1.ff.3.bias\", \"decoder.net.attn_layers.layers.6.0.0.weight\", \"decoder.net.attn_layers.layers.6.0.0.bias\", \"decoder.net.attn_layers.layers.7.0.0.weight\", \"decoder.net.attn_layers.layers.7.0.0.bias\", \"decoder.net.attn_layers.layers.8.0.0.weight\", \"decoder.net.attn_layers.layers.8.0.0.bias\", \"decoder.net.attn_layers.layers.8.1.ff.0.proj.weight\", \"decoder.net.attn_layers.layers.8.1.ff.0.proj.bias\", \"decoder.net.attn_layers.layers.8.1.ff.3.weight\", \"decoder.net.attn_layers.layers.8.1.ff.3.bias\", \"decoder.net.attn_layers.layers.9.0.0.weight\", \"decoder.net.attn_layers.layers.9.0.0.bias\", \"decoder.net.attn_layers.layers.10.0.0.weight\", \"decoder.net.attn_layers.layers.10.0.0.bias\", \"decoder.net.attn_layers.layers.11.0.0.weight\", \"decoder.net.attn_layers.layers.11.0.0.bias\", \"decoder.net.attn_layers.layers.11.1.ff.0.proj.weight\", \"decoder.net.attn_layers.layers.11.1.ff.0.proj.bias\", \"decoder.net.attn_layers.layers.11.1.ff.3.weight\", \"decoder.net.attn_layers.layers.11.1.ff.3.bias\". \n\tUnexpected key(s) in state_dict: \"decoder.net.token_emb.weight\", \"decoder.net.attn_layers.layers.0.0.weight\", \"decoder.net.attn_layers.layers.0.0.bias\", \"decoder.net.attn_layers.layers.0.1.to_out.0.bias\", \"decoder.net.attn_layers.layers.1.0.weight\", \"decoder.net.attn_layers.layers.1.0.bias\", \"decoder.net.attn_layers.layers.1.1.to_out.0.bias\", \"decoder.net.attn_layers.layers.2.0.weight\", \"decoder.net.attn_layers.layers.2.0.bias\", \"decoder.net.attn_layers.layers.2.1.net.0.proj.weight\", \"decoder.net.attn_layers.layers.2.1.net.0.proj.bias\", \"decoder.net.attn_layers.layers.2.1.net.2.weight\", \"decoder.net.attn_layers.layers.2.1.net.2.bias\", \"decoder.net.attn_layers.layers.3.0.weight\", \"decoder.net.attn_layers.layers.3.0.bias\", \"decoder.net.attn_layers.layers.3.1.to_out.0.bias\", \"decoder.net.attn_layers.layers.4.0.weight\", \"decoder.net.attn_layers.layers.4.0.bias\", \"decoder.net.attn_layers.layers.4.1.to_out.0.bias\", \"decoder.net.attn_layers.layers.5.0.weight\", \"decoder.net.attn_layers.layers.5.0.bias\", \"decoder.net.attn_layers.layers.5.1.net.0.proj.weight\", \"decoder.net.attn_layers.layers.5.1.net.0.proj.bias\", \"decoder.net.attn_layers.layers.5.1.net.2.weight\", \"decoder.net.attn_layers.layers.5.1.net.2.bias\", \"decoder.net.attn_layers.layers.6.0.weight\", \"decoder.net.attn_layers.layers.6.0.bias\", \"decoder.net.attn_layers.layers.6.1.to_out.0.bias\", \"decoder.net.attn_layers.layers.7.0.weight\", \"decoder.net.attn_layers.layers.7.0.bias\", \"decoder.net.attn_layers.layers.7.1.to_out.0.bias\", \"decoder.net.attn_layers.layers.8.0.weight\", \"decoder.net.attn_layers.layers.8.0.bias\", \"decoder.net.attn_layers.layers.8.1.net.0.proj.weight\", \"decoder.net.attn_layers.layers.8.1.net.0.proj.bias\", \"decoder.net.attn_layers.layers.8.1.net.2.weight\", \"decoder.net.attn_layers.layers.8.1.net.2.bias\", \"decoder.net.attn_layers.layers.9.0.weight\", \"decoder.net.attn_layers.layers.9.0.bias\", \"decoder.net.attn_layers.layers.9.1.to_out.0.bias\", \"decoder.net.attn_layers.layers.10.0.weight\", \"decoder.net.attn_layers.layers.10.0.bias\", \"decoder.net.attn_layers.layers.10.1.to_out.0.bias\", \"decoder.net.attn_layers.layers.11.0.weight\", \"decoder.net.attn_layers.layers.11.0.bias\", \"decoder.net.attn_layers.layers.11.1.net.0.proj.weight\", \"decoder.net.attn_layers.layers.11.1.net.0.proj.bias\", \"decoder.net.attn_layers.layers.11.1.net.2.weight\", \"decoder.net.attn_layers.layers.11.1.net.2.bias\". ",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[2], line 21\u001B[0m\n\u001B[1;32m     19\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpix2tex\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m cli \u001B[38;5;28;01mas\u001B[39;00m pix2tex\n\u001B[1;32m     20\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mPIL\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Image\n\u001B[0;32m---> 21\u001B[0m model \u001B[38;5;241m=\u001B[39m \u001B[43mpix2tex\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mLatexOCR\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     23\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mIPython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdisplay\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m HTML, Math\n\u001B[1;32m     24\u001B[0m display(HTML(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m<script src=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mhttps://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     25\u001B[0m              \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlatest.js?config=default\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m></script>\u001B[39m\u001B[38;5;124m\"\u001B[39m))\n",
      "File \u001B[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/contextlib.py:79\u001B[0m, in \u001B[0;36mContextDecorator.__call__.<locals>.inner\u001B[0;34m(*args, **kwds)\u001B[0m\n\u001B[1;32m     76\u001B[0m \u001B[38;5;129m@wraps\u001B[39m(func)\n\u001B[1;32m     77\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21minner\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwds):\n\u001B[1;32m     78\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_recreate_cm():\n\u001B[0;32m---> 79\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/research/LaTeX-OCR/pix2tex/cli.py:84\u001B[0m, in \u001B[0;36mLatexOCR.__init__\u001B[0;34m(self, arguments)\u001B[0m\n\u001B[1;32m     82\u001B[0m     download_checkpoints()\n\u001B[1;32m     83\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel \u001B[38;5;241m=\u001B[39m get_model(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs)\n\u001B[0;32m---> 84\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload_state_dict\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43margs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcheckpoint\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmap_location\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43margs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     85\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel\u001B[38;5;241m.\u001B[39meval()\n\u001B[1;32m     87\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mimage_resizer.pth\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;129;01min\u001B[39;00m os\u001B[38;5;241m.\u001B[39mlistdir(os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mdirname(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs\u001B[38;5;241m.\u001B[39mcheckpoint)) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m arguments\u001B[38;5;241m.\u001B[39mno_resize:\n",
      "File \u001B[0;32m~/research/LaTeX-OCR/venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1671\u001B[0m, in \u001B[0;36mModule.load_state_dict\u001B[0;34m(self, state_dict, strict)\u001B[0m\n\u001B[1;32m   1666\u001B[0m         error_msgs\u001B[38;5;241m.\u001B[39minsert(\n\u001B[1;32m   1667\u001B[0m             \u001B[38;5;241m0\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mMissing key(s) in state_dict: \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m. \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mformat(\n\u001B[1;32m   1668\u001B[0m                 \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m, \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mjoin(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mformat(k) \u001B[38;5;28;01mfor\u001B[39;00m k \u001B[38;5;129;01min\u001B[39;00m missing_keys)))\n\u001B[1;32m   1670\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(error_msgs) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m-> 1671\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mError(s) in loading state_dict for \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;130;01m\\t\u001B[39;00m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mformat(\n\u001B[1;32m   1672\u001B[0m                        \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;130;01m\\t\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mjoin(error_msgs)))\n\u001B[1;32m   1673\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001B[0;31mRuntimeError\u001B[0m: Error(s) in loading state_dict for Model:\n\tMissing key(s) in state_dict: \"decoder.net.token_emb.emb.weight\", \"decoder.net.attn_layers.layers.0.0.0.weight\", \"decoder.net.attn_layers.layers.0.0.0.bias\", \"decoder.net.attn_layers.layers.1.0.0.weight\", \"decoder.net.attn_layers.layers.1.0.0.bias\", \"decoder.net.attn_layers.layers.2.0.0.weight\", \"decoder.net.attn_layers.layers.2.0.0.bias\", \"decoder.net.attn_layers.layers.2.1.ff.0.proj.weight\", \"decoder.net.attn_layers.layers.2.1.ff.0.proj.bias\", \"decoder.net.attn_layers.layers.2.1.ff.3.weight\", \"decoder.net.attn_layers.layers.2.1.ff.3.bias\", \"decoder.net.attn_layers.layers.3.0.0.weight\", \"decoder.net.attn_layers.layers.3.0.0.bias\", \"decoder.net.attn_layers.layers.4.0.0.weight\", \"decoder.net.attn_layers.layers.4.0.0.bias\", \"decoder.net.attn_layers.layers.5.0.0.weight\", \"decoder.net.attn_layers.layers.5.0.0.bias\", \"decoder.net.attn_layers.layers.5.1.ff.0.proj.weight\", \"decoder.net.attn_layers.layers.5.1.ff.0.proj.bias\", \"decoder.net.attn_layers.layers.5.1.ff.3.weight\", \"decoder.net.attn_layers.layers.5.1.ff.3.bias\", \"decoder.net.attn_layers.layers.6.0.0.weight\", \"decoder.net.attn_layers.layers.6.0.0.bias\", \"decoder.net.attn_layers.layers.7.0.0.weight\", \"decoder.net.attn_layers.layers.7.0.0.bias\", \"decoder.net.attn_layers.layers.8.0.0.weight\", \"decoder.net.attn_layers.layers.8.0.0.bias\", \"decoder.net.attn_layers.layers.8.1.ff.0.proj.weight\", \"decoder.net.attn_layers.layers.8.1.ff.0.proj.bias\", \"decoder.net.attn_layers.layers.8.1.ff.3.weight\", \"decoder.net.attn_layers.layers.8.1.ff.3.bias\", \"decoder.net.attn_layers.layers.9.0.0.weight\", \"decoder.net.attn_layers.layers.9.0.0.bias\", \"decoder.net.attn_layers.layers.10.0.0.weight\", \"decoder.net.attn_layers.layers.10.0.0.bias\", \"decoder.net.attn_layers.layers.11.0.0.weight\", \"decoder.net.attn_layers.layers.11.0.0.bias\", \"decoder.net.attn_layers.layers.11.1.ff.0.proj.weight\", \"decoder.net.attn_layers.layers.11.1.ff.0.proj.bias\", \"decoder.net.attn_layers.layers.11.1.ff.3.weight\", \"decoder.net.attn_layers.layers.11.1.ff.3.bias\". \n\tUnexpected key(s) in state_dict: \"decoder.net.token_emb.weight\", \"decoder.net.attn_layers.layers.0.0.weight\", \"decoder.net.attn_layers.layers.0.0.bias\", \"decoder.net.attn_layers.layers.0.1.to_out.0.bias\", \"decoder.net.attn_layers.layers.1.0.weight\", \"decoder.net.attn_layers.layers.1.0.bias\", \"decoder.net.attn_layers.layers.1.1.to_out.0.bias\", \"decoder.net.attn_layers.layers.2.0.weight\", \"decoder.net.attn_layers.layers.2.0.bias\", \"decoder.net.attn_layers.layers.2.1.net.0.proj.weight\", \"decoder.net.attn_layers.layers.2.1.net.0.proj.bias\", \"decoder.net.attn_layers.layers.2.1.net.2.weight\", \"decoder.net.attn_layers.layers.2.1.net.2.bias\", \"decoder.net.attn_layers.layers.3.0.weight\", \"decoder.net.attn_layers.layers.3.0.bias\", \"decoder.net.attn_layers.layers.3.1.to_out.0.bias\", \"decoder.net.attn_layers.layers.4.0.weight\", \"decoder.net.attn_layers.layers.4.0.bias\", \"decoder.net.attn_layers.layers.4.1.to_out.0.bias\", \"decoder.net.attn_layers.layers.5.0.weight\", \"decoder.net.attn_layers.layers.5.0.bias\", \"decoder.net.attn_layers.layers.5.1.net.0.proj.weight\", \"decoder.net.attn_layers.layers.5.1.net.0.proj.bias\", \"decoder.net.attn_layers.layers.5.1.net.2.weight\", \"decoder.net.attn_layers.layers.5.1.net.2.bias\", \"decoder.net.attn_layers.layers.6.0.weight\", \"decoder.net.attn_layers.layers.6.0.bias\", \"decoder.net.attn_layers.layers.6.1.to_out.0.bias\", \"decoder.net.attn_layers.layers.7.0.weight\", \"decoder.net.attn_layers.layers.7.0.bias\", \"decoder.net.attn_layers.layers.7.1.to_out.0.bias\", \"decoder.net.attn_layers.layers.8.0.weight\", \"decoder.net.attn_layers.layers.8.0.bias\", \"decoder.net.attn_layers.layers.8.1.net.0.proj.weight\", \"decoder.net.attn_layers.layers.8.1.net.0.proj.bias\", \"decoder.net.attn_layers.layers.8.1.net.2.weight\", \"decoder.net.attn_layers.layers.8.1.net.2.bias\", \"decoder.net.attn_layers.layers.9.0.weight\", \"decoder.net.attn_layers.layers.9.0.bias\", \"decoder.net.attn_layers.layers.9.1.to_out.0.bias\", \"decoder.net.attn_layers.layers.10.0.weight\", \"decoder.net.attn_layers.layers.10.0.bias\", \"decoder.net.attn_layers.layers.10.1.to_out.0.bias\", \"decoder.net.attn_layers.layers.11.0.weight\", \"decoder.net.attn_layers.layers.11.0.bias\", \"decoder.net.attn_layers.layers.11.1.net.0.proj.weight\", \"decoder.net.attn_layers.layers.11.1.net.0.proj.bias\", \"decoder.net.attn_layers.layers.11.1.net.2.weight\", \"decoder.net.attn_layers.layers.11.1.net.2.bias\". "
     ]
    }
   ],
   "source": [
    "#@title Setup\n",
    "%reload_ext autoreload\n",
    "%autoreload\n",
    "import PIL\n",
    "!pip install Pillow -U -qq\n",
    "if int(PIL.__version__[0]) < 9:\n",
    "    print('Mandatory restart: Execute this cell again!')\n",
    "    import os\n",
    "    os.kill(os.getpid(), 9)\n",
    "!pip install pix2tex -qq\n",
    "!pip install opencv-python-headless==4.1.2.30 -U -qq\n",
    "\n",
    "def upload_files():\n",
    "  from google.colab import files\n",
    "  from io import BytesIO\n",
    "  uploaded = files.upload()\n",
    "  return [(name, BytesIO(b)) for name, b in uploaded.items()]\n",
    "\n",
    "from pix2tex import cli as pix2tex\n",
    "from PIL import Image\n",
    "model = pix2tex.LatexOCR()\n",
    "\n",
    "from IPython.display import HTML, Math\n",
    "display(HTML(\"<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/\"\n",
    "             \"latest.js?config=default'></script>\"))\n",
    "table = r'\\begin{array} {l|l} %s  \\end{array}'"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "imgs = upload_files()\n",
    "predictions = []\n",
    "for name, f in imgs:\n",
    "    img = Image.open(f)\n",
    "    math = model(img)\n",
    "    print(math)\n",
    "    predictions.append('\\\\mathrm{%s} & \\\\displaystyle{%s}'%(name, math))\n",
    "Math(table%'\\\\\\\\'.join(predictions))"
   ],
   "metadata": {
    "id": "CjrR3O07u3uH",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 3,
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google.colab'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[3], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m imgs \u001B[38;5;241m=\u001B[39m \u001B[43mupload_files\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      2\u001B[0m predictions \u001B[38;5;241m=\u001B[39m []\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m name, f \u001B[38;5;129;01min\u001B[39;00m imgs:\n",
      "Cell \u001B[0;32mIn[2], line 14\u001B[0m, in \u001B[0;36mupload_files\u001B[0;34m()\u001B[0m\n\u001B[1;32m     13\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mupload_files\u001B[39m():\n\u001B[0;32m---> 14\u001B[0m   \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mgoogle\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcolab\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m files\n\u001B[1;32m     15\u001B[0m   \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mio\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m BytesIO\n\u001B[1;32m     16\u001B[0m   uploaded \u001B[38;5;241m=\u001B[39m files\u001B[38;5;241m.\u001B[39mupload()\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'google.colab'"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "ZqCH-4XoCkMO",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}